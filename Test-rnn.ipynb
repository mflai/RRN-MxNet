{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary dependencies\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn, rnn\n",
    "import mxnet.ndarray as F\n",
    "import logging\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "py_new = True #  Checks for python version\n",
    "if sys.version[:1] == '2':\n",
    "    py_new = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check context available and load the necessary context (cpu/gpu)\n",
    "try:\n",
    "    a = mx.nd.ones((2,3), mx.gpu(0))\n",
    "    context = mx.gpu(0)\n",
    "except:\n",
    "    context = mx.cpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # For Python 3.0 and later\n",
    "    from urllib.request import urlopen\n",
    "except ImportError:\n",
    "    # Fall back to Python 2's urllib2\n",
    "    from urllib2 import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAndStoreFile(url):\n",
    "    import os\n",
    "    try:\n",
    "        # For Python 3.0 and later\n",
    "        from urllib.request import urlopen\n",
    "    except ImportError:\n",
    "        # Fall back to Python 2's urllib2\n",
    "        from urllib2 import urlopen\n",
    "    if not os.path.isdir(\"data\"):\n",
    "        response = urlopen(url)\n",
    "        #  There might be problem decoding the response\n",
    "        #  Please checkt he data folder and the file before trying the execrise\n",
    "        #  in windows operating system  'data = response.read()' is enough\n",
    "        data = response.read().decode('UTF-8')\n",
    "        os.mkdir(\"data\")\n",
    "        with open(\"data/nietzsche.txt\", \"w+\") as f:\n",
    "            f.write(data)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
    "readAndStoreFile(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600893\n"
     ]
    }
   ],
   "source": [
    "# loading https://s3.amazonaws.com/text-datasets/nietzsche.txt \n",
    "# You can load anyother text you want\n",
    "# (https://cs.stanford.edu/people/karpathy/char-rnn/)\n",
    "if py_new:\n",
    "    with open(\"data/nietzsche.txt\", errors='ignore') as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "     with open(\"data/nietzsche.txt\") as f:\n",
    "        text = f.read()\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 85\n"
     ]
    }
   ],
   "source": [
    "# total of characters in dataset\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeros for padding\n",
    "chars.insert(0, \"\\0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"\\'(),-.0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxy'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(chars[1:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps character to unique index e.g. {a:1,b:2....}\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "# maps indices to character (1:a,2:b ....)\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the dataset into index\n",
    "idx = [char_indices[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600893\n"
     ]
    }
   ],
   "source": [
    "print(len(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not gro'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the mapping\n",
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our unrolled RNN\n",
    "\n",
    "In this model we map 3 inputs to one output. Later we will design rnn with n inputs to n inputs (sequence to sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input for neural network(our basic rnn has 3 inputs, n samples)\n",
    "cs = 3\n",
    "c1_dat = [idx[i] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c2_dat = [idx[i+1] for i in range(0, len(idx)-1-cs, cs)]\n",
    "c3_dat = [idx[i+2] for i in range(0, len(idx)-1-cs, cs)]\n",
    "# the output of rnn network (single vector)\n",
    "c4_dat = [idx[i+3] for i in range(0, len(idx)-1-cs, cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking the inputs to form (3 input features )\n",
    "x1 = np.stack(c1_dat[:-2])\n",
    "x2 = np.stack(c2_dat[:-2])\n",
    "x3 = np.stack(c3_dat[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the output (1 X N data points)\n",
    "y = np.stack(c4_dat[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200295, 3)\n"
     ]
    }
   ],
   "source": [
    "col_concat = np.array([x1, x2, x3])\n",
    "t_col_concat = col_concat.T\n",
    "print(t_col_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our sample inputs for the model\n",
    "x1_nd = mx.nd.array(x1)\n",
    "x2_nd = mx.nd.array(x2)\n",
    "x3_nd = mx.nd.array(x3)\n",
    "sample_input = mx.nd.array([[x1[0], x2[0], x3[0]], [x1[1], x2[1], x3[1]]])\n",
    "\n",
    "simple_train_data = mx.nd.array(t_col_concat)\n",
    "simple_label_data = mx.nd.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batchsize as 32, so input is of form 32 X 3\n",
    "# output is 32 X 1\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def get_batch(source, label_data, i, batch_size=32):\n",
    "    bb_size = min(batch_size, source.shape[0] - 1 - i)\n",
    "    data = source[i: i + bb_size]\n",
    "    target = label_data[i: i + bb_size]\n",
    "    # print(target.shape)\n",
    "    return data, target.reshape((-1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "test_bat, test_target = get_batch(simple_train_data,\n",
    "                                  simple_label_data, 5, batch_size)\n",
    "print(test_bat.shape)\n",
    "print(test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/unRolled_rnn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple UnRollredRNN_Model\n",
    "from mxnet.gluon import Block, nn\n",
    "from mxnet import ndarray as F\n",
    "\n",
    "\n",
    "class UnRolledRNN_Model(Block):\n",
    "    def __init__(self, vocab_size, num_embed, num_hidden, **kwargs):\n",
    "        super(UnRolledRNN_Model, self).__init__(**kwargs)\n",
    "        self.num_embed = num_embed\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # use name_scope to give child Blocks appropriate names.\n",
    "        # It also allows sharing Parameters between Blocks recursively.\n",
    "        with self.name_scope():\n",
    "            self.encoder = nn.Embedding(self.vocab_size, self.num_embed)\n",
    "            self.dense1 = nn.Dense(num_hidden, activation='relu', flatten=True)\n",
    "            self.dense2 = nn.Dense(num_hidden, activation='relu', flatten=True)\n",
    "            self.dense3 = nn.Dense(vocab_size, flatten=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emd = self.encoder(inputs)\n",
    "        # print( emd.shape )\n",
    "        # since the input is shape (batch_size, input(3 characters) )\n",
    "        # we need to extract 0th, 1st, 2nd character from each batch\n",
    "        character1 = emd[:, 0, :]\n",
    "        character2 = emd[:, 1, :]\n",
    "        character3 = emd[:, 2, :]\n",
    "        # green arrow in diagram for character 1\n",
    "        c1_hidden = self.dense1(character1)\n",
    "        # green arrow in diagram for character 2\n",
    "        c2_hidden = self.dense1(character2)\n",
    "        # green arrow in diagram for character 3\n",
    "        c3_hidden = self.dense1(character3)\n",
    "        # yellow arrow in diagram\n",
    "        c1_hidden_2 = self.dense2(c1_hidden)\n",
    "        addition_result = F.add(c2_hidden, c1_hidden_2)  # Total c1 + c2\n",
    "        addition_hidden = self.dense2(addition_result)  # the yellow arrow\n",
    "        addition_result_2 = F.add(addition_hidden, c3_hidden)  # Total c1 + c2\n",
    "        final_output = self.dense3(addition_result_2)\n",
    "        return final_output\n",
    "\n",
    "\n",
    "vocab_size = len(chars) + 1  # the vocabsize\n",
    "num_embed = 30\n",
    "num_hidden = 256\n",
    "# model creation\n",
    "simple_model = UnRolledRNN_Model(vocab_size, num_embed, num_hidden)\n",
    "# model initilisation\n",
    "simple_model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "trainer = gluon.Trainer(simple_model.collect_params(), 'adam')\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "# sample input shape is of size (2x3)\n",
    "# output = simple_model (sample_input)\n",
    "# sample out shape should be (3*87). 87 is our vocab size\n",
    "# print('the output shape',output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory already exists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check point file\n",
    "try:\n",
    "    os.makedirs('checkpoints')\n",
    "except:\n",
    "    print(\"directory already exists\")\n",
    "filename_unrolled_rnn = \"checkpoints/rnn_gluon_abc.params\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actual training\n",
    "def UnRolledRNNtrain(train_data, label_data, batch_size=32, epochs=10):\n",
    "    epochs = epochs\n",
    "    smoothing_constant = .01\n",
    "    for e in range(epochs):\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, batch_size)):\n",
    "            data, target = get_batch(train_data, label_data, i, batch_size)\n",
    "            data = data.as_in_context(context)\n",
    "            target = target.as_in_context(context)\n",
    "            with autograd.record():\n",
    "                output = simple_model(data)\n",
    "                L = loss(output, target)\n",
    "            L.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "\n",
    "            ##########################\n",
    "            #  Keep a moving average of the losses\n",
    "            ##########################\n",
    "            if ibatch == 128:\n",
    "                curr_loss = mx.nd.mean(L).asscalar()\n",
    "                moving_loss = 0\n",
    "                moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                               else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "                print(\"Epoch %s. Loss: %s, moving_loss %s\" % (e, curr_loss, moving_loss))\n",
    "    simple_model.save_params(filename_unrolled_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.57198, moving_loss 0.0257198143005\n",
      "Epoch 1. Loss: 2.07404, moving_loss 0.0207403850555\n",
      "Epoch 2. Loss: 2.08466, moving_loss 0.0208466291428\n",
      "Epoch 3. Loss: 2.09426, moving_loss 0.020942568779\n",
      "Epoch 4. Loss: 2.09758, moving_loss 0.0209757971764\n",
      "Epoch 5. Loss: 2.1124, moving_loss 0.0211240243912\n",
      "Epoch 6. Loss: 2.09077, moving_loss 0.0209077453613\n",
      "Epoch 7. Loss: 2.08054, moving_loss 0.0208053588867\n",
      "Epoch 8. Loss: 2.06016, moving_loss 0.0206015586853\n",
      "Epoch 9. Loss: 2.0602, moving_loss 0.0206020212173\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "UnRolledRNNtrain(simple_train_data, simple_label_data, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model back\n",
    "simple_model.load_params(filename_unrolled_rnn, ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model\n",
    "def evaluate(input_string):\n",
    "    idx = [char_indices[c] for c in input_string]\n",
    "    sample_input = mx.nd.array([[idx[0], idx[1], idx[2]]], ctx=context)\n",
    "    output = simple_model(sample_input)\n",
    "    index = mx.nd.argmax(output, axis=1)\n",
    "    return index.asnumpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predicted answer is  e\n"
     ]
    }
   ],
   "source": [
    "# predictions\n",
    "begin_char = 'lov'\n",
    "answer = evaluate(begin_char)\n",
    "print('the predicted answer is ', indices_char[answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mx.gluon.rnn.SequentialRNNCell()\n",
    "with model.name_scope():\n",
    "    model.add(mx.gluon.rnn.LSTMCell(20))\n",
    "    model.add(mx.gluon.rnn.LSTMCell(20))\n",
    "states = model.begin_state(batch_size=32)\n",
    "inputs = mx.nd.random.uniform(shape=(5, 32, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character RNN using gluon/lstm api\n",
    "\n",
    "Training sequence 2 sequence models using Gluon API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create model objects.\n",
    "class GluonRNNModel(gluon.Block):\n",
    "    \"\"\"A model with an encoder, recurrent layer, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, vocab_size, num_embed, num_hidden,\n",
    "                 num_layers, dropout=0.5, **kwargs):\n",
    "        super(GluonRNNModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "            self.encoder = nn.Embedding(vocab_size, num_embed,\n",
    "                                        weight_initializer=mx.init.Uniform(0.1))\n",
    "\n",
    "            if mode == 'lstm':\n",
    "                #  we create a LSTM layer with certain number of hidden LSTM cell and layers\n",
    "                #  in our example num_hidden is 1000 and num of layers is 2\n",
    "                #  The input to the LSTM will only be passed during the forward pass (see forward function below)\n",
    "                self.rnn = rnn.LSTM(num_hidden, num_layers, dropout=dropout,\n",
    "                                    input_size=num_embed)\n",
    "            elif mode == 'gru':\n",
    "                #  we create a GRU layer with certain number of hidden GRU cell and layers\n",
    "                #  in our example num_hidden is 1000 and num of layers is 2\n",
    "                #  The input to the GRU will only be passed during the forward pass (see forward function below)\n",
    "                self.rnn = rnn.GRU(num_hidden, num_layers, dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            else:\n",
    "                #  we create a vanilla RNN layer with certain number of hidden vanilla RNN cell and layers\n",
    "                #  in our example num_hidden is 1000 and num of layers is 2\n",
    "                #  The input to the vanilla will only be passed during the forward pass (see forward function below)\n",
    "                self.rnn = rnn.RNN(num_hidden, num_layers, activation='relu', dropout=dropout,\n",
    "                                   input_size=num_embed)\n",
    "            self.decoder = nn.Dense(vocab_size, in_units=num_hidden)\n",
    "            self.num_hidden = num_hidden\n",
    "\n",
    "    #  define the forward pass of the neural network\n",
    "    def forward(self, inputs, hidden):\n",
    "        emb = self.encoder(inputs)\n",
    "        #  emb, hidden are the inputs to the hidden \n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        #  the ouput from the hidden layer to passed to drop out layer\n",
    "        output = self.drop(output)\n",
    "        #  print('output forward',output.shape)\n",
    "        #  Then the output is flattened to a shape of with 1000 columns \n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "\n",
    "    # Initial state of netork\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the lstm\n",
    "mode = 'lstm'\n",
    "# number of characters in vocab_size\n",
    "vocab_size = len(chars) + 1\n",
    "embedsize = 50\n",
    "hididen_units = 1000\n",
    "number_layers = 2\n",
    "clip = 0.2\n",
    "epochs = 200  # use 200 epochs for good result\n",
    "batch_size = 32\n",
    "seq_length = 100  # sequence length\n",
    "dropout = 0.4\n",
    "log_interval = 64\n",
    "# checkpoints/gluonlstm_2 (prepared for seq_lenght 100, 200 epochs)\n",
    "rnn_save = 'checkpoints/gluonlstm_200.params'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GluonRNNModel\n",
    "model = GluonRNNModel(mode, vocab_size, embedsize, hididen_units,\n",
    "                      number_layers, dropout)\n",
    "# initalise the weights of models to random weights\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "# Adam trainer\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam')\n",
    "# softmax cros entropy loss\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepares rnn batches\n",
    "# The batch will be of shape is (num_example * batch_size) because of RNN uses sequences of input     x\n",
    "# for example if we use (a1,a2,a3) as one input sequence , (b1,b2,b3) as another input sequence and (c1,c2,c3)\n",
    "# if we have batch of 3, then at timestep '1'  we only have (a1,b1.c1) as input, at timestep '2' we have (a2,b2,c2) as input...\n",
    "# hence the batchsize is of order \n",
    "# In feedforward we use (batch_size, num_example)\n",
    "def rnn_batch(data, batch_size):\n",
    "    \"\"\"Reshape data into (num_example, batch_size)\"\"\"\n",
    "    nbatch = data.shape[0] // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    data = data.reshape((batch_size, nbatch)).T\n",
    "    return data\n",
    "\n",
    "idx_nd = mx.nd.array(idx)\n",
    "# convert the idex of characters\n",
    "train_data_rnn_gluon = rnn_batch(idx_nd, batch_size).as_in_context(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the batch\n",
    "def get_batch(source, i, seq):\n",
    "    seq_len = min(seq, source.shape[0] - 1 - i)\n",
    "    data = source[i: i + seq_len]\n",
    "    target = source[i + 1: i + 1 + seq_len]\n",
    "    return data, target.reshape((-1,))\n",
    "\n",
    "\n",
    "# detach the hidden state, so we dont accidentally compute gradients\n",
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [i.detach() for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGluonRNN(epochs, train_data, seq=seq_length):\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        hidden = model.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=context)\n",
    "        for ibatch, i in enumerate(range(0, train_data.shape[0] - 1, seq_length)):\n",
    "            data, target = get_batch(train_data, i, seq)\n",
    "            hidden = detach(hidden)\n",
    "            with autograd.record():\n",
    "                output, hidden = model(data, hidden)\n",
    "                L = loss(output, target) # this is total loss associated with seq_length\n",
    "                L.backward()\n",
    "\n",
    "            grads = [i.grad(context) for i in model.collect_params().values()]\n",
    "            # Here gradient is for the whole batch.\n",
    "            # So we multiply max_norm by batch_size and seq_length to balance it.\n",
    "            gluon.utils.clip_global_norm(grads, clip * seq_length * batch_size)\n",
    "\n",
    "            trainer.step(batch_size)\n",
    "            total_L += mx.nd.sum(L).asscalar()\n",
    "\n",
    "            if ibatch % log_interval == 0 and ibatch > 0:\n",
    "                cur_L = total_L / seq_length / batch_size / log_interval\n",
    "                print('[Epoch %d Batch %d] loss %.2f' % (epoch + 1, ibatch, cur_L))\n",
    "                total_L = 0.0\n",
    "        model.save_params(rnn_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the train data shape is (18777, 32)\n"
     ]
    }
   ],
   "source": [
    "print('the train data shape is', train_data_rnn_gluon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 64] loss 3.15\n",
      "[Epoch 1 Batch 128] loss 2.41\n",
      "[Epoch 2 Batch 64] loss 2.05\n",
      "[Epoch 2 Batch 128] loss 1.84\n",
      "[Epoch 3 Batch 64] loss 1.70\n",
      "[Epoch 3 Batch 128] loss 1.58\n",
      "[Epoch 4 Batch 64] loss 1.54\n",
      "[Epoch 4 Batch 128] loss 1.45\n",
      "[Epoch 5 Batch 64] loss 1.45\n",
      "[Epoch 5 Batch 128] loss 1.38\n",
      "[Epoch 6 Batch 64] loss 1.38\n",
      "[Epoch 6 Batch 128] loss 1.32\n",
      "[Epoch 7 Batch 64] loss 1.33\n",
      "[Epoch 7 Batch 128] loss 1.27\n",
      "[Epoch 8 Batch 64] loss 1.29\n",
      "[Epoch 8 Batch 128] loss 1.23\n",
      "[Epoch 9 Batch 64] loss 1.25\n",
      "[Epoch 9 Batch 128] loss 1.19\n",
      "[Epoch 10 Batch 64] loss 1.21\n",
      "[Epoch 10 Batch 128] loss 1.16\n",
      "[Epoch 11 Batch 64] loss 1.18\n",
      "[Epoch 11 Batch 128] loss 1.13\n",
      "[Epoch 12 Batch 64] loss 1.15\n",
      "[Epoch 12 Batch 128] loss 1.09\n",
      "[Epoch 13 Batch 64] loss 1.11\n",
      "[Epoch 13 Batch 128] loss 1.06\n",
      "[Epoch 14 Batch 64] loss 1.08\n",
      "[Epoch 14 Batch 128] loss 1.03\n",
      "[Epoch 15 Batch 64] loss 1.05\n",
      "[Epoch 15 Batch 128] loss 1.00\n",
      "[Epoch 16 Batch 64] loss 1.02\n",
      "[Epoch 16 Batch 128] loss 0.97\n",
      "[Epoch 17 Batch 64] loss 0.99\n",
      "[Epoch 17 Batch 128] loss 0.94\n",
      "[Epoch 18 Batch 64] loss 0.96\n",
      "[Epoch 18 Batch 128] loss 0.92\n",
      "[Epoch 19 Batch 64] loss 0.93\n",
      "[Epoch 19 Batch 128] loss 0.89\n",
      "[Epoch 20 Batch 64] loss 0.91\n",
      "[Epoch 20 Batch 128] loss 0.86\n",
      "[Epoch 21 Batch 64] loss 0.88\n",
      "[Epoch 21 Batch 128] loss 0.85\n",
      "[Epoch 22 Batch 64] loss 0.86\n",
      "[Epoch 22 Batch 128] loss 0.82\n",
      "[Epoch 23 Batch 64] loss 0.84\n",
      "[Epoch 23 Batch 128] loss 0.80\n",
      "[Epoch 24 Batch 64] loss 0.81\n",
      "[Epoch 24 Batch 128] loss 0.79\n",
      "[Epoch 25 Batch 64] loss 0.79\n",
      "[Epoch 25 Batch 128] loss 0.76\n",
      "[Epoch 26 Batch 64] loss 0.78\n",
      "[Epoch 26 Batch 128] loss 0.75\n",
      "[Epoch 27 Batch 64] loss 0.76\n",
      "[Epoch 27 Batch 128] loss 0.73\n",
      "[Epoch 28 Batch 64] loss 0.74\n",
      "[Epoch 28 Batch 128] loss 0.72\n",
      "[Epoch 29 Batch 64] loss 0.72\n",
      "[Epoch 29 Batch 128] loss 0.70\n",
      "[Epoch 30 Batch 64] loss 0.71\n",
      "[Epoch 30 Batch 128] loss 0.69\n",
      "[Epoch 31 Batch 64] loss 0.70\n",
      "[Epoch 31 Batch 128] loss 0.67\n",
      "[Epoch 32 Batch 64] loss 0.68\n",
      "[Epoch 32 Batch 128] loss 0.66\n",
      "[Epoch 33 Batch 64] loss 0.67\n",
      "[Epoch 33 Batch 128] loss 0.65\n",
      "[Epoch 34 Batch 64] loss 0.65\n",
      "[Epoch 34 Batch 128] loss 0.64\n",
      "[Epoch 35 Batch 64] loss 0.64\n",
      "[Epoch 35 Batch 128] loss 0.63\n",
      "[Epoch 36 Batch 64] loss 0.63\n",
      "[Epoch 36 Batch 128] loss 0.61\n",
      "[Epoch 37 Batch 64] loss 0.62\n",
      "[Epoch 37 Batch 128] loss 0.60\n",
      "[Epoch 38 Batch 64] loss 0.61\n",
      "[Epoch 38 Batch 128] loss 0.60\n",
      "[Epoch 39 Batch 64] loss 0.60\n",
      "[Epoch 39 Batch 128] loss 0.58\n",
      "[Epoch 40 Batch 64] loss 0.60\n",
      "[Epoch 40 Batch 128] loss 0.57\n",
      "[Epoch 41 Batch 64] loss 0.58\n",
      "[Epoch 41 Batch 128] loss 0.57\n",
      "[Epoch 42 Batch 64] loss 0.58\n",
      "[Epoch 42 Batch 128] loss 0.56\n",
      "[Epoch 43 Batch 64] loss 0.57\n",
      "[Epoch 43 Batch 128] loss 0.55\n",
      "[Epoch 44 Batch 64] loss 0.56\n",
      "[Epoch 44 Batch 128] loss 0.54\n",
      "[Epoch 45 Batch 64] loss 0.55\n",
      "[Epoch 45 Batch 128] loss 0.54\n",
      "[Epoch 46 Batch 64] loss 0.54\n",
      "[Epoch 46 Batch 128] loss 0.53\n",
      "[Epoch 47 Batch 64] loss 0.54\n",
      "[Epoch 47 Batch 128] loss 0.52\n",
      "[Epoch 48 Batch 64] loss 0.53\n",
      "[Epoch 48 Batch 128] loss 0.52\n",
      "[Epoch 49 Batch 64] loss 0.52\n",
      "[Epoch 49 Batch 128] loss 0.51\n",
      "[Epoch 50 Batch 64] loss 0.52\n",
      "[Epoch 50 Batch 128] loss 0.51\n",
      "[Epoch 51 Batch 64] loss 0.51\n",
      "[Epoch 51 Batch 128] loss 0.50\n",
      "[Epoch 52 Batch 64] loss 0.51\n",
      "[Epoch 52 Batch 128] loss 0.49\n",
      "[Epoch 53 Batch 64] loss 0.50\n",
      "[Epoch 53 Batch 128] loss 0.49\n",
      "[Epoch 54 Batch 64] loss 0.50\n",
      "[Epoch 54 Batch 128] loss 0.48\n",
      "[Epoch 55 Batch 64] loss 0.49\n",
      "[Epoch 55 Batch 128] loss 0.48\n",
      "[Epoch 56 Batch 64] loss 0.49\n",
      "[Epoch 56 Batch 128] loss 0.47\n",
      "[Epoch 57 Batch 64] loss 0.48\n",
      "[Epoch 57 Batch 128] loss 0.47\n",
      "[Epoch 58 Batch 64] loss 0.48\n",
      "[Epoch 58 Batch 128] loss 0.47\n",
      "[Epoch 59 Batch 64] loss 0.48\n",
      "[Epoch 59 Batch 128] loss 0.46\n",
      "[Epoch 60 Batch 64] loss 0.47\n",
      "[Epoch 60 Batch 128] loss 0.45\n",
      "[Epoch 61 Batch 64] loss 0.46\n",
      "[Epoch 61 Batch 128] loss 0.45\n",
      "[Epoch 62 Batch 64] loss 0.46\n",
      "[Epoch 62 Batch 128] loss 0.44\n",
      "[Epoch 63 Batch 64] loss 0.46\n",
      "[Epoch 63 Batch 128] loss 0.44\n",
      "[Epoch 64 Batch 64] loss 0.45\n",
      "[Epoch 64 Batch 128] loss 0.44\n",
      "[Epoch 65 Batch 64] loss 0.45\n",
      "[Epoch 65 Batch 128] loss 0.43\n",
      "[Epoch 66 Batch 64] loss 0.45\n",
      "[Epoch 66 Batch 128] loss 0.43\n",
      "[Epoch 67 Batch 64] loss 0.44\n",
      "[Epoch 67 Batch 128] loss 0.43\n",
      "[Epoch 68 Batch 64] loss 0.44\n",
      "[Epoch 68 Batch 128] loss 0.42\n",
      "[Epoch 69 Batch 64] loss 0.44\n",
      "[Epoch 69 Batch 128] loss 0.42\n",
      "[Epoch 70 Batch 64] loss 0.43\n",
      "[Epoch 70 Batch 128] loss 0.41\n",
      "[Epoch 71 Batch 64] loss 0.43\n",
      "[Epoch 71 Batch 128] loss 0.41\n",
      "[Epoch 72 Batch 64] loss 0.42\n",
      "[Epoch 72 Batch 128] loss 0.41\n",
      "[Epoch 73 Batch 64] loss 0.42\n",
      "[Epoch 73 Batch 128] loss 0.40\n",
      "[Epoch 74 Batch 64] loss 0.42\n",
      "[Epoch 74 Batch 128] loss 0.40\n",
      "[Epoch 75 Batch 64] loss 0.41\n",
      "[Epoch 75 Batch 128] loss 0.40\n",
      "[Epoch 76 Batch 64] loss 0.41\n",
      "[Epoch 76 Batch 128] loss 0.40\n",
      "[Epoch 77 Batch 64] loss 0.41\n",
      "[Epoch 77 Batch 128] loss 0.39\n",
      "[Epoch 78 Batch 64] loss 0.41\n",
      "[Epoch 78 Batch 128] loss 0.39\n",
      "[Epoch 79 Batch 64] loss 0.41\n",
      "[Epoch 79 Batch 128] loss 0.39\n",
      "[Epoch 80 Batch 64] loss 0.40\n",
      "[Epoch 80 Batch 128] loss 0.39\n",
      "[Epoch 81 Batch 64] loss 0.40\n",
      "[Epoch 81 Batch 128] loss 0.38\n",
      "[Epoch 82 Batch 64] loss 0.39\n",
      "[Epoch 82 Batch 128] loss 0.38\n",
      "[Epoch 83 Batch 64] loss 0.39\n",
      "[Epoch 83 Batch 128] loss 0.38\n",
      "[Epoch 84 Batch 64] loss 0.39\n",
      "[Epoch 84 Batch 128] loss 0.37\n",
      "[Epoch 85 Batch 64] loss 0.39\n",
      "[Epoch 85 Batch 128] loss 0.37\n",
      "[Epoch 86 Batch 64] loss 0.38\n",
      "[Epoch 86 Batch 128] loss 0.37\n",
      "[Epoch 87 Batch 64] loss 0.38\n",
      "[Epoch 87 Batch 128] loss 0.37\n",
      "[Epoch 88 Batch 64] loss 0.38\n",
      "[Epoch 88 Batch 128] loss 0.37\n",
      "[Epoch 89 Batch 64] loss 0.38\n",
      "[Epoch 89 Batch 128] loss 0.36\n",
      "[Epoch 90 Batch 64] loss 0.38\n",
      "[Epoch 90 Batch 128] loss 0.36\n",
      "[Epoch 91 Batch 64] loss 0.37\n",
      "[Epoch 91 Batch 128] loss 0.36\n",
      "[Epoch 92 Batch 64] loss 0.37\n",
      "[Epoch 92 Batch 128] loss 0.35\n",
      "[Epoch 93 Batch 64] loss 0.37\n",
      "[Epoch 93 Batch 128] loss 0.35\n",
      "[Epoch 94 Batch 64] loss 0.37\n",
      "[Epoch 94 Batch 128] loss 0.35\n",
      "[Epoch 95 Batch 64] loss 0.36\n",
      "[Epoch 95 Batch 128] loss 0.35\n",
      "[Epoch 96 Batch 64] loss 0.36\n",
      "[Epoch 96 Batch 128] loss 0.34\n",
      "[Epoch 97 Batch 64] loss 0.36\n",
      "[Epoch 97 Batch 128] loss 0.35\n",
      "[Epoch 98 Batch 64] loss 0.36\n",
      "[Epoch 98 Batch 128] loss 0.34\n",
      "[Epoch 99 Batch 64] loss 0.36\n",
      "[Epoch 99 Batch 128] loss 0.34\n",
      "[Epoch 100 Batch 64] loss 0.35\n",
      "[Epoch 100 Batch 128] loss 0.34\n",
      "[Epoch 101 Batch 64] loss 0.35\n",
      "[Epoch 101 Batch 128] loss 0.34\n",
      "[Epoch 102 Batch 64] loss 0.35\n",
      "[Epoch 102 Batch 128] loss 0.33\n",
      "[Epoch 103 Batch 64] loss 0.35\n",
      "[Epoch 103 Batch 128] loss 0.33\n",
      "[Epoch 104 Batch 64] loss 0.35\n",
      "[Epoch 104 Batch 128] loss 0.33\n",
      "[Epoch 105 Batch 64] loss 0.35\n",
      "[Epoch 105 Batch 128] loss 0.33\n",
      "[Epoch 106 Batch 64] loss 0.34\n",
      "[Epoch 106 Batch 128] loss 0.33\n",
      "[Epoch 107 Batch 64] loss 0.34\n",
      "[Epoch 107 Batch 128] loss 0.32\n",
      "[Epoch 108 Batch 64] loss 0.34\n",
      "[Epoch 108 Batch 128] loss 0.32\n",
      "[Epoch 109 Batch 64] loss 0.34\n",
      "[Epoch 109 Batch 128] loss 0.32\n",
      "[Epoch 110 Batch 64] loss 0.34\n",
      "[Epoch 110 Batch 128] loss 0.32\n",
      "[Epoch 111 Batch 64] loss 0.33\n",
      "[Epoch 111 Batch 128] loss 0.32\n",
      "[Epoch 112 Batch 64] loss 0.34\n",
      "[Epoch 112 Batch 128] loss 0.32\n",
      "[Epoch 113 Batch 64] loss 0.33\n",
      "[Epoch 113 Batch 128] loss 0.31\n",
      "[Epoch 114 Batch 64] loss 0.33\n",
      "[Epoch 114 Batch 128] loss 0.31\n",
      "[Epoch 115 Batch 64] loss 0.33\n",
      "[Epoch 115 Batch 128] loss 0.31\n",
      "[Epoch 116 Batch 64] loss 0.33\n",
      "[Epoch 116 Batch 128] loss 0.31\n",
      "[Epoch 117 Batch 64] loss 0.32\n",
      "[Epoch 117 Batch 128] loss 0.31\n",
      "[Epoch 118 Batch 64] loss 0.33\n",
      "[Epoch 118 Batch 128] loss 0.30\n",
      "[Epoch 119 Batch 64] loss 0.32\n",
      "[Epoch 119 Batch 128] loss 0.30\n",
      "[Epoch 120 Batch 64] loss 0.32\n",
      "[Epoch 120 Batch 128] loss 0.30\n",
      "[Epoch 121 Batch 64] loss 0.32\n",
      "[Epoch 121 Batch 128] loss 0.30\n",
      "[Epoch 122 Batch 64] loss 0.32\n",
      "[Epoch 122 Batch 128] loss 0.30\n",
      "[Epoch 123 Batch 64] loss 0.32\n",
      "[Epoch 123 Batch 128] loss 0.30\n",
      "[Epoch 124 Batch 64] loss 0.32\n",
      "[Epoch 124 Batch 128] loss 0.29\n",
      "[Epoch 125 Batch 64] loss 0.31\n",
      "[Epoch 125 Batch 128] loss 0.30\n",
      "[Epoch 126 Batch 64] loss 0.31\n",
      "[Epoch 126 Batch 128] loss 0.29\n",
      "[Epoch 127 Batch 64] loss 0.31\n",
      "[Epoch 127 Batch 128] loss 0.29\n",
      "[Epoch 128 Batch 64] loss 0.31\n",
      "[Epoch 128 Batch 128] loss 0.29\n",
      "[Epoch 129 Batch 64] loss 0.31\n",
      "[Epoch 129 Batch 128] loss 0.29\n",
      "[Epoch 130 Batch 64] loss 0.31\n",
      "[Epoch 130 Batch 128] loss 0.29\n",
      "[Epoch 131 Batch 64] loss 0.31\n",
      "[Epoch 131 Batch 128] loss 0.28\n",
      "[Epoch 132 Batch 64] loss 0.30\n",
      "[Epoch 132 Batch 128] loss 0.28\n",
      "[Epoch 133 Batch 64] loss 0.31\n",
      "[Epoch 133 Batch 128] loss 0.28\n",
      "[Epoch 134 Batch 64] loss 0.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 134 Batch 128] loss 0.29\n",
      "[Epoch 135 Batch 64] loss 0.30\n",
      "[Epoch 135 Batch 128] loss 0.28\n",
      "[Epoch 136 Batch 64] loss 0.30\n",
      "[Epoch 136 Batch 128] loss 0.28\n",
      "[Epoch 137 Batch 64] loss 0.30\n",
      "[Epoch 137 Batch 128] loss 0.28\n",
      "[Epoch 138 Batch 64] loss 0.30\n",
      "[Epoch 138 Batch 128] loss 0.28\n",
      "[Epoch 139 Batch 64] loss 0.29\n",
      "[Epoch 139 Batch 128] loss 0.28\n",
      "[Epoch 140 Batch 64] loss 0.29\n",
      "[Epoch 140 Batch 128] loss 0.28\n",
      "[Epoch 141 Batch 64] loss 0.29\n",
      "[Epoch 141 Batch 128] loss 0.27\n",
      "[Epoch 142 Batch 64] loss 0.29\n",
      "[Epoch 142 Batch 128] loss 0.27\n",
      "[Epoch 143 Batch 64] loss 0.29\n",
      "[Epoch 143 Batch 128] loss 0.27\n",
      "[Epoch 144 Batch 64] loss 0.29\n",
      "[Epoch 144 Batch 128] loss 0.27\n",
      "[Epoch 145 Batch 64] loss 0.29\n",
      "[Epoch 145 Batch 128] loss 0.27\n",
      "[Epoch 146 Batch 64] loss 0.29\n",
      "[Epoch 146 Batch 128] loss 0.27\n",
      "[Epoch 147 Batch 64] loss 0.29\n",
      "[Epoch 147 Batch 128] loss 0.27\n",
      "[Epoch 148 Batch 64] loss 0.28\n",
      "[Epoch 148 Batch 128] loss 0.26\n",
      "[Epoch 149 Batch 64] loss 0.28\n",
      "[Epoch 149 Batch 128] loss 0.26\n",
      "[Epoch 150 Batch 64] loss 0.28\n",
      "[Epoch 150 Batch 128] loss 0.26\n",
      "[Epoch 151 Batch 64] loss 0.28\n",
      "[Epoch 151 Batch 128] loss 0.26\n",
      "[Epoch 152 Batch 64] loss 0.28\n",
      "[Epoch 152 Batch 128] loss 0.26\n",
      "[Epoch 153 Batch 64] loss 0.28\n",
      "[Epoch 153 Batch 128] loss 0.26\n",
      "[Epoch 154 Batch 64] loss 0.27\n",
      "[Epoch 154 Batch 128] loss 0.26\n",
      "[Epoch 155 Batch 64] loss 0.28\n",
      "[Epoch 155 Batch 128] loss 0.26\n",
      "[Epoch 156 Batch 64] loss 0.28\n",
      "[Epoch 156 Batch 128] loss 0.26\n",
      "[Epoch 157 Batch 64] loss 0.27\n",
      "[Epoch 157 Batch 128] loss 0.26\n",
      "[Epoch 158 Batch 64] loss 0.27\n",
      "[Epoch 158 Batch 128] loss 0.25\n",
      "[Epoch 159 Batch 64] loss 0.27\n",
      "[Epoch 159 Batch 128] loss 0.25\n",
      "[Epoch 160 Batch 64] loss 0.27\n",
      "[Epoch 160 Batch 128] loss 0.25\n",
      "[Epoch 161 Batch 64] loss 0.27\n",
      "[Epoch 161 Batch 128] loss 0.25\n",
      "[Epoch 162 Batch 64] loss 0.27\n",
      "[Epoch 162 Batch 128] loss 0.25\n",
      "[Epoch 163 Batch 64] loss 0.27\n",
      "[Epoch 163 Batch 128] loss 0.25\n",
      "[Epoch 164 Batch 64] loss 0.27\n",
      "[Epoch 164 Batch 128] loss 0.25\n",
      "[Epoch 165 Batch 64] loss 0.26\n",
      "[Epoch 165 Batch 128] loss 0.25\n",
      "[Epoch 166 Batch 64] loss 0.26\n",
      "[Epoch 166 Batch 128] loss 0.25\n",
      "[Epoch 167 Batch 64] loss 0.26\n",
      "[Epoch 167 Batch 128] loss 0.25\n",
      "[Epoch 168 Batch 64] loss 0.26\n",
      "[Epoch 168 Batch 128] loss 0.24\n",
      "[Epoch 169 Batch 64] loss 0.26\n",
      "[Epoch 169 Batch 128] loss 0.25\n",
      "[Epoch 170 Batch 64] loss 0.26\n",
      "[Epoch 170 Batch 128] loss 0.24\n",
      "[Epoch 171 Batch 64] loss 0.26\n",
      "[Epoch 171 Batch 128] loss 0.24\n",
      "[Epoch 172 Batch 64] loss 0.26\n",
      "[Epoch 172 Batch 128] loss 0.24\n",
      "[Epoch 173 Batch 64] loss 0.25\n",
      "[Epoch 173 Batch 128] loss 0.24\n",
      "[Epoch 174 Batch 64] loss 0.25\n",
      "[Epoch 174 Batch 128] loss 0.24\n",
      "[Epoch 175 Batch 64] loss 0.25\n",
      "[Epoch 175 Batch 128] loss 0.24\n",
      "[Epoch 176 Batch 64] loss 0.25\n",
      "[Epoch 176 Batch 128] loss 0.24\n",
      "[Epoch 177 Batch 64] loss 0.25\n",
      "[Epoch 177 Batch 128] loss 0.24\n",
      "[Epoch 178 Batch 64] loss 0.25\n",
      "[Epoch 178 Batch 128] loss 0.24\n",
      "[Epoch 179 Batch 64] loss 0.25\n",
      "[Epoch 179 Batch 128] loss 0.23\n",
      "[Epoch 180 Batch 64] loss 0.25\n",
      "[Epoch 180 Batch 128] loss 0.23\n",
      "[Epoch 181 Batch 64] loss 0.25\n",
      "[Epoch 181 Batch 128] loss 0.23\n",
      "[Epoch 182 Batch 64] loss 0.25\n",
      "[Epoch 182 Batch 128] loss 0.23\n",
      "[Epoch 183 Batch 64] loss 0.25\n",
      "[Epoch 183 Batch 128] loss 0.23\n",
      "[Epoch 184 Batch 64] loss 0.25\n",
      "[Epoch 184 Batch 128] loss 0.23\n",
      "[Epoch 185 Batch 64] loss 0.25\n",
      "[Epoch 185 Batch 128] loss 0.23\n",
      "[Epoch 186 Batch 64] loss 0.24\n",
      "[Epoch 186 Batch 128] loss 0.23\n",
      "[Epoch 187 Batch 64] loss 0.24\n",
      "[Epoch 187 Batch 128] loss 0.23\n",
      "[Epoch 188 Batch 64] loss 0.24\n",
      "[Epoch 188 Batch 128] loss 0.23\n",
      "[Epoch 189 Batch 64] loss 0.24\n",
      "[Epoch 189 Batch 128] loss 0.22\n",
      "[Epoch 190 Batch 64] loss 0.24\n",
      "[Epoch 190 Batch 128] loss 0.22\n"
     ]
    }
   ],
   "source": [
    "# The train data shape\n",
    "trainGluonRNN(epochs, train_data_rnn_gluon, seq=seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_params(rnn_save, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates a seqtoseq model over input string\n",
    "def evaluate_seq2seq(model, input_string, seq_length, batch_size):\n",
    "    idx = [char_indices[c] for c in input_string]\n",
    "    if(len(input_string) != seq_length):\n",
    "        raise ValueError(\"input string should be equal to sequence length\")\n",
    "    hidden = model.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=context)\n",
    "    sample_input = mx.nd.array(np.array([idx[0:seq_length]]).T, ctx=context)\n",
    "    output, hidden = model(sample_input, hidden)\n",
    "    index = mx.nd.argmax(output, axis=1)\n",
    "    index = index.asnumpy()\n",
    "    return [indices_char[char] for char in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps the input sequence to output sequence\n",
    "def mapInput(input_str, output_str):\n",
    "    for i, _ in enumerate(input_str):\n",
    "        partial_input = input_str[:i+1]\n",
    "        partial_output = output_str[i:i+1]\n",
    "        print(partial_input + \"->\" + partial_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "p->h\n",
      "pr->i\n",
      "pro->b\n",
      "prob->a\n",
      "proba->b\n",
      "probab->l\n",
      "probabl->y\n",
      "probably-> \n",
      "probably ->s\n",
      "probably t->h\n",
      "probably th->i\n",
      "probably the-> \n",
      "probably the ->b\n",
      "probably the t->r\n",
      "probably the ti->m\n",
      "probably the tim->e\n",
      "probably the time-> \n",
      "probably the time ->o\n",
      "probably the time i->s\n",
      "probably the time is-> \n",
      "probably the time is ->a\n",
      "probably the time is a->t\n",
      "probably the time is at-> \n",
      "probably the time is at ->h\n",
      "probably the time is at h->a\n",
      "probably the time is at ha->n\n",
      "probably the time is at han->d\n",
      "probably the time is at hand-> \n",
      "probably the time is at hand ->w\n",
      "probably the time is at hand w->h\n",
      "probably the time is at hand wh->e\n",
      "probably the time is at hand whe->n\n",
      "probably the time is at hand when-> \n",
      "probably the time is at hand when ->i\n",
      "probably the time is at hand when i->t\n",
      "probably the time is at hand when it-> \n",
      "probably the time is at hand when it ->w\n",
      "probably the time is at hand when it w->i\n",
      "probably the time is at hand when it wi->l\n",
      "probably the time is at hand when it wil->l\n",
      "probably the time is at hand when it will-> \n",
      "probably the time is at hand when it will ->b\n",
      "probably the time is at hand when it will b->e\n",
      "probably the time is at hand when it will be-> \n",
      "probably the time is at hand when it will be ->o\n",
      "probably the time is at hand when it will be o->n\n",
      "probably the time is at hand when it will be on->c\n",
      "probably the time is at hand when it will be onc->e\n",
      "probably the time is at hand when it will be once->\n",
      "\n",
      "probably the time is at hand when it will be once ->a\n",
      "probably the time is at hand when it will be once a->n\n",
      "probably the time is at hand when it will be once an->d\n",
      "probably the time is at hand when it will be once and-> \n",
      "probably the time is at hand when it will be once and ->a\n",
      "probably the time is at hand when it will be once and a->g\n",
      "probably the time is at hand when it will be once and ag->a\n",
      "probably the time is at hand when it will be once and aga->i\n",
      "probably the time is at hand when it will be once and agai->n\n",
      "probably the time is at hand when it will be once and again-> \n",
      "probably the time is at hand when it will be once and again ->u\n",
      "probably the time is at hand when it will be once and again u->n\n",
      "probably the time is at hand when it will be once and again un->d\n",
      "probably the time is at hand when it will be once and again und->e\n",
      "probably the time is at hand when it will be once and again unde->r\n",
      "probably the time is at hand when it will be once and again under->s\n",
      "probably the time is at hand when it will be once and again unders->t\n",
      "probably the time is at hand when it will be once and again underst->o\n",
      "probably the time is at hand when it will be once and again understo->o\n",
      "probably the time is at hand when it will be once and again understoo->d\n",
      "probably the time is at hand when it will be once and again understood-> \n",
      "probably the time is at hand when it will be once and again understood ->W\n",
      "probably the time is at hand when it will be once and again understood W->H\n",
      "probably the time is at hand when it will be once and again understood WH->A\n",
      "probably the time is at hand when it will be once and again understood WHA->T\n",
      "probably the time is at hand when it will be once and again understood WHAT-> \n",
      "probably the time is at hand when it will be once and again understood WHAT ->h\n",
      "probably the time is at hand when it will be once and again understood WHAT h->a\n",
      "probably the time is at hand when it will be once and again understood WHAT ha->s\n",
      "probably the time is at hand when it will be once and again understood WHAT has-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has ->a\n",
      "probably the time is at hand when it will be once and again understood WHAT has a->c\n",
      "probably the time is at hand when it will be once and again understood WHAT has ac->t\n",
      "probably the time is at hand when it will be once and again understood WHAT has act->u\n",
      "probably the time is at hand when it will be once and again understood WHAT has actu->a\n",
      "probably the time is at hand when it will be once and again understood WHAT has actua->l\n",
      "probably the time is at hand when it will be once and again understood WHAT has actual->l\n",
      "probably the time is at hand when it will be once and again understood WHAT has actuall->y\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has actually ->s\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually s->u\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually su->f\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suf->f\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suff->i\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suffi->c\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suffic->e\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually suffice->d\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed-> \n",
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed ->f\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed a->n\n",
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed an->d\n"
     ]
    }
   ],
   "source": [
    "test_input = 'probably the time is at hand when it will be once and again understood WHAT has actually sufficed an'\n",
    "print(len(test_input))\n",
    "result = evaluate_seq2seq(model, test_input, seq_length, 1)\n",
    "mapInput(test_input, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a nietzsche like text generator\n",
    "import sys\n",
    "\n",
    "\n",
    "def generate_random_text(model, input_string, seq_length, batch_size, sentence_length):\n",
    "    count = 0\n",
    "    new_string = ''\n",
    "    cp_input_string = input_string\n",
    "    hidden = model.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=context)\n",
    "    while count < sentence_length:\n",
    "        idx = [char_indices[c] for c in input_string]\n",
    "        if(len(input_string) != seq_length):\n",
    "            print(len(input_string))\n",
    "            raise ValueError('there was a error in the input ')\n",
    "        sample_input = mx.nd.array(np.array([idx[0:seq_length]]).T, ctx=context)\n",
    "        output, hidden = model(sample_input, hidden)\n",
    "        index = mx.nd.argmax(output, axis=1)\n",
    "        index = index.asnumpy()\n",
    "        count = count + 1\n",
    "        new_string = new_string + indices_char[index[-1]]\n",
    "        input_string = input_string[1:] + indices_char[index[-1]]\n",
    "    print(cp_input_string + new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably the time is at hand when it will be once and again understood WHAT has actually sufficed and finally such a man, things conditions for his task; this task itself demands something\n",
      "else--it requires him TO CREATE VALUES. The philosophical workers, after\n",
      "the excellent pattern of Kant and Hege\n"
     ]
    }
   ],
   "source": [
    "generate_random_text(model, \"probably the time is at hand when it will be once and again understood WHAT has actually sufficed an\", seq_length, 1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
